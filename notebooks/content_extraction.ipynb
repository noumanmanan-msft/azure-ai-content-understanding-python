{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Content from Your File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the Content Understanding API to extract semantic content from multimodal files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that provides functions to interact with the Content Understanding API. Prior to the official release of the Content Understanding SDK, it serves as a lightweight SDK.\n",
    ">\n",
    "> Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with the details from your Azure AI Service.\n",
    "\n",
    "> ⚠️ Important:\n",
    "You must update the code below to use your preferred Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments in the code and modify those sections accordingly.\n",
    "Skipping this step may cause the sample to not run correctly.\n",
    "\n",
    "> ⚠️ Note: While using a subscription key is supported, it is strongly recommended to use a token provider with Azure Active Directory (AAD) for enhanced security in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in your \".env\" file if not using token authentication\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\")\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_ENDPOINT,\n",
    "    api_version=AZURE_AI_API_VERSION,\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment the following line if using subscription key\n",
    "    # subscription_key=AZURE_AI_API_KEY,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/content_extraction\",  # This header is used for sample usage telemetry; please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Utility function to save images\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "def save_image(image_id: str, response):\n",
    "    raw_image = client.get_image_from_analyze_operation(analyze_response=response,\n",
    "        image_id=image_id\n",
    "    )\n",
    "    image = Image.open(BytesIO(raw_image))\n",
    "    # To display the image, uncomment the following line:\n",
    "    # image.show()\n",
    "    Path(\".cache\").mkdir(exist_ok=True)\n",
    "    image.save(f\".cache/{image_id}.jpg\", \"JPEG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Content\n",
    "\n",
    "The Content Understanding API extracts all textual content from a given document file. In addition to text extraction, it performs a comprehensive layout analysis to identify and categorize tables and figures within the document. The output is presented in a structured markdown format, ensuring clarity and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_SAMPLE_FILE = '../data/invoice.pdf'\n",
    "ANALYZER_ID = 'prebuilt-documentAnalyzer'\n",
    "\n",
    "# Analyze document file\n",
    "response = client.begin_analyze(ANALYZER_ID, file_location=ANALYZER_SAMPLE_FILE)\n",
    "result_json = client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The markdown output contains detailed layout information, which is especially useful for Retrieval-Augmented Generation (RAG) scenarios. You can paste the markdown into a viewer such as Visual Studio Code to preview the layout structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_json[\"result\"][\"contents\"][0][\"markdown\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can access layout information including `words` and `lines` within the `pages` node, paragraph details under `paragraphs`, and tables listed in the `tables` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result_json[\"result\"][\"contents\"][0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This output helps you retrieve structural information about the tables embedded within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result_json[\"result\"][\"contents\"][0][\"tables\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Content\n",
    "The API provides detailed analysis of spoken language, enabling developers to build applications such as voice recognition, customer service analytics, and conversational AI. The output structure facilitates extraction and analysis of different conversation components for further processing or insights.\n",
    "\n",
    "Key features include:\n",
    "1. **Speaker Identification:** Each phrase is linked to a specific speaker (e.g., \"Speaker 2\"), enabling clear differentiation in multi-participant conversations.\n",
    "2. **Timing Information:** Each transcription includes precise timing data.\n",
    "    - startTimeMs: The time (in milliseconds) when the phrase begins.\n",
    "    - endTimeMs: The time (in milliseconds) when the phrase ends.\n",
    "    This information is crucial for applications like video subtitles and audio-text synchronization.\n",
    "3. **Text Content:** The actual spoken text, such as \"Thank you for calling Woodgrove Travel,\" representing the main transcription.\n",
    "4. **Confidence Score:** Each phrase has a confidence score (e.g., 0.933) indicating transcription reliability.\n",
    "5. **Word-Level Breakdown:** Detailed timing for each word supports advanced speech analysis and improvements in speech recognition.\n",
    "6. **Locale Specification:** The locale (e.g., \"en-US\") informs the transcription process of regional dialects and pronunciation nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_SAMPLE_FILE = '../data/audio.wav'\n",
    "ANALYZER_ID = 'prebuilt-audioAnalyzer'\n",
    "\n",
    "# Analyze audio file\n",
    "response = client.begin_analyze(ANALYZER_ID, file_location=ANALYZER_SAMPLE_FILE)\n",
    "result_json = client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Content\n",
    "The video output provides detailed metadata about audiovisual content, specifically video shots. Key features include:\n",
    "\n",
    "1. **Shot Information:** Each shot has a start and end time with a unique identifier. For example, 'Shot 0:0.0 to 0:2.800' includes a transcript and key frames.\n",
    "2. **Transcript:** Audio transcripts formatted in WEBVTT facilitate synchronization with video playback. It captures spoken content and specifies the timing of the dialogue.\n",
    "3. **Key Frames:** A collection of key frames (images) represent important moments in the video, allowing visualization of specific timestamps.\n",
    "4. **Description:** Each shot includes a descriptive summary, providing context about the visuals presented. This helps in understanding the scene or subject matter without watching the video.\n",
    "5. **Audio Visual Metadata:** Information such as video dimensions (width, height), type (audiovisual), and key frame timestamps.\n",
    "6. **Transcript Phrases:** Specific dialog phrases with timing and speaker attribution enhance usability for applications like closed captioning and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_SAMPLE_FILE = '../data/FlightSimulator.mp4'\n",
    "ANALYZER_ID = 'prebuilt-videoAnalyzer'\n",
    "\n",
    "# Analyze video file\n",
    "response = client.begin_analyze(ANALYZER_ID, file_location=ANALYZER_SAMPLE_FILE)\n",
    "result_json = client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result_json, indent=2))\n",
    "\n",
    "# Save keyframes (optional)\n",
    "keyframe_ids = set()\n",
    "result_data = result_json.get(\"result\", {})\n",
    "contents = result_data.get(\"contents\", [])\n",
    "\n",
    "# Extract keyframe IDs from markdown content\n",
    "for content in contents:\n",
    "    markdown_content = content.get(\"markdown\", \"\")\n",
    "    if isinstance(markdown_content, str):\n",
    "        keyframe_ids.update(re.findall(r\"(keyFrame\\.\\d+)\\.jpg\", markdown_content))\n",
    "\n",
    "# Output unique keyframe IDs\n",
    "print(\"Unique Keyframe IDs:\", keyframe_ids)\n",
    "\n",
    "# Save all keyframe images\n",
    "for keyframe_id in keyframe_ids:\n",
    "    save_image(keyframe_id, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Content with Face Recognition\n",
    "This is a gated feature. To enable it, please follow the registration process outlined in [Azure AI Resource Face Gating](https://learn.microsoft.com/en-us/legal/cognitive-services/computer-vision/limited-access-identity?context=%2Fazure%2Fai-services%2Fcomputer-vision%2Fcontext%2Fcontext#registration-process).\n",
    "In the registration form, select:\n",
    "`[Video Indexer] Facial Identification (1:N or 1:1 matching)` to search for faces within media or entertainment video archives and generate metadata for these use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_SAMPLE_FILE = '../data/FlightSimulator.mp4'\n",
    "ANALYZER_ID = 'prebuilt-videoAnalyzer'\n",
    "\n",
    "# Analyze video file with face recognition\n",
    "response = client.begin_analyze(ANALYZER_ID, file_location=ANALYZER_SAMPLE_FILE)\n",
    "result_json = client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and Save Key Frames and Face Thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sets to store unique face IDs and keyframe IDs\n",
    "face_ids = set()\n",
    "keyframe_ids = set()\n",
    "\n",
    "# Safely extract face IDs and keyframe IDs from content\n",
    "result_data = result_json.get(\"result\", {})\n",
    "contents = result_data.get(\"contents\", [])\n",
    "\n",
    "for content in contents:\n",
    "    # Extract face IDs if \"faces\" field exists and is a list\n",
    "    faces = content.get(\"faces\", [])\n",
    "    if isinstance(faces, list):\n",
    "        for face in faces:\n",
    "            face_id = face.get(\"faceId\")\n",
    "            if face_id:\n",
    "                face_ids.add(f\"face.{face_id}\")\n",
    "\n",
    "    # Extract keyframe IDs from \"markdown\" if present and a string\n",
    "    markdown_content = content.get(\"markdown\", \"\")\n",
    "    if isinstance(markdown_content, str):\n",
    "        keyframe_ids.update(re.findall(r\"(keyFrame\\.\\d+)\\.jpg\", markdown_content))\n",
    "\n",
    "# Display unique face and keyframe IDs\n",
    "print(\"Unique Face IDs:\", face_ids)\n",
    "print(\"Unique Keyframe IDs:\", keyframe_ids)\n",
    "\n",
    "# Save all face images\n",
    "for face_id in face_ids:\n",
    "    save_image(face_id, response)\n",
    "\n",
    "# Save all keyframe images\n",
    "for keyframe_id in keyframe_ids:\n",
    "    save_image(keyframe_id, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
